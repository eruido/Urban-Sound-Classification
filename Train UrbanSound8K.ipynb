{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data source for wav files\n",
    "dataSourcePath = 'UrbanSound8K/augmented/train'\n",
    "trainSourcePath = 'UrbanSound8K/augmented/test'\n",
    "\n",
    "# Total wav records for training the model, will be updated by the program\n",
    "totalRecordCount = 0\n",
    "\n",
    "# Total classification class for your model (e.g. if you plan to classify 10 different sounds, then the value is 10)\n",
    "totalLabel = 10\n",
    "\n",
    "# model parameters for training\n",
    "batchSize = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData():\n",
    "    dataSet = []\n",
    "    totalCount = 0\n",
    "    progressThreashold = 100\n",
    "    os.walk(dataSourcePath)\n",
    "    print('============= Start import train data set =============')\n",
    "    for root, _, files in os.walk(dataSourcePath):\n",
    "        for file in files:\n",
    "            fileName, fileExtension = os.path.splitext(file)\n",
    "            if fileExtension != '.wav': continue\n",
    "            wavFilePath = os.path.join(root, file)\n",
    "            y, sr = librosa.load(wavFilePath, duration=2.97)\n",
    "            ps = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "            if ps.shape != (128, 128): continue\n",
    "            if totalCount % progressThreashold == 0:\n",
    "                print('Importing data count:{}'.format(totalCount))\n",
    "            # extract the class label from the FileName\n",
    "            label = fileName.split('-')[1]\n",
    "            dataSet.append( (ps, label) )\n",
    "            totalCount += 1\n",
    "    global totalRecordCount\n",
    "    totalRecordCount = totalCount\n",
    "\n",
    "    # Import train data set\n",
    "    print('============= Start import test data set =============')\n",
    "    totalCount = 0\n",
    "    for root, _, files in os.walk(trainSourcePath):\n",
    "        for file in files:\n",
    "            fileName, fileExtension = os.path.splitext(file)\n",
    "            if fileExtension != '.wav': continue\n",
    "            wavFilePath = os.path.join(root, file)\n",
    "            y, sr = librosa.load(wavFilePath, duration=2.97)\n",
    "            ps = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "            if ps.shape != (128, 128): continue\n",
    "            if totalCount % progressThreashold == 0:\n",
    "                print('Importing data count:{}'.format(totalCount))\n",
    "            # extract the class label from the FileName\n",
    "            label = fileName.split('-')[1]\n",
    "            dataSet.append( (ps, label) )\n",
    "            totalCount += 1\n",
    "\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(dataset):\n",
    "        \n",
    "    # print('Total Train Data Count: {}'.format(totalRecordCount))\n",
    "    trainDataEndIndex = int(totalRecordCount)\n",
    "    # random.shuffle(dataset)\n",
    "\n",
    "    train = dataset[:trainDataEndIndex]\n",
    "    test = dataset[trainDataEndIndex:]\n",
    "\n",
    "    print('Total training data:{}'.format(len(train)))\n",
    "    print('Total test data:{}'.format(len(test)))\n",
    "\n",
    "    # Get the data (128, 128) and label from tuple\n",
    "    X_train, y_train = zip(*train)\n",
    "    X_test, y_test = zip(*test)\n",
    "\n",
    "    # Reshape for CNN input\n",
    "    X_train = np.array([x.reshape( (128, 128, 1) ) for x in X_train])\n",
    "    X_test = np.array([x.reshape( (128, 128, 1) ) for x in X_test])\n",
    "\n",
    "    # One-Hot encoding for classes\n",
    "    y_train = np.array(keras.utils.to_categorical(y_train, totalLabel))\n",
    "    y_test = np.array(keras.utils.to_categorical(y_test, totalLabel))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Model Input\n",
    "    input_shape=(128, 128, 1)\n",
    "\n",
    "    # Using CNN to build model\n",
    "    # 24 depths 128 - 5 + 1 = 124 x 124 x 24\n",
    "    model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "    # 31 x 62 x 24\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # 27 x 58 x 48\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "\n",
    "    # 6 x 29 x 48\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # 2 x 25 x 48\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(totalLabel))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[precision, recall])\n",
    "\n",
    "    model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batchSize,\n",
    "        validation_data= (X_test, y_test),\n",
    "    )\n",
    "\n",
    "    score = model.evaluate(\n",
    "        x=X_test,\n",
    "        y=y_test)\n",
    "\n",
    "    timestr = time.strftime('%Y%m%d-%H%M%S')\n",
    "    modelName = 'sound-classification-{}.h5'.format(timestr)\n",
    "    model.save('models/{}'.format(modelName))\n",
    "\n",
    "    print('Model exported and finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \n",
    "    Only computes a batch-wise average of precision.   \n",
    "    \n",
    "    Computes the precision, a metric for multi-label classification of \n",
    "    how many selected items are relevant.  \n",
    "    \"\"\"    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())   \n",
    "    return precision   \n",
    " \n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of    \n",
    "    how many relevant items are selected.  \n",
    "    \"\"\"    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))  \n",
    "    recall = true_positives / (possible_positives + K.epsilon())   \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Start import train data set =============\n",
      "Importing data count:0\n",
      "Importing data count:100\n",
      "Importing data count:200\n",
      "Importing data count:300\n",
      "Importing data count:400\n",
      "Importing data count:500\n",
      "Importing data count:600\n",
      "Importing data count:700\n",
      "Importing data count:800\n",
      "Importing data count:900\n",
      "Importing data count:1000\n",
      "Importing data count:1100\n",
      "Importing data count:1200\n",
      "Importing data count:1300\n",
      "Importing data count:1400\n",
      "Importing data count:1500\n",
      "Importing data count:1600\n",
      "Importing data count:1700\n",
      "Importing data count:1800\n",
      "Importing data count:1900\n",
      "Importing data count:2000\n",
      "Importing data count:2100\n",
      "Importing data count:2200\n",
      "Importing data count:2300\n",
      "Importing data count:2400\n",
      "Importing data count:2500\n",
      "Importing data count:2600\n",
      "Importing data count:2700\n",
      "Importing data count:2800\n",
      "Importing data count:2900\n",
      "Importing data count:3000\n",
      "Importing data count:3100\n",
      "Importing data count:3200\n",
      "Importing data count:3300\n",
      "Importing data count:3400\n",
      "Importing data count:3500\n",
      "Importing data count:3600\n",
      "Importing data count:3700\n",
      "Importing data count:3800\n",
      "Importing data count:3900\n",
      "Importing data count:4000\n",
      "Importing data count:4100\n",
      "Importing data count:4200\n",
      "Importing data count:4300\n",
      "Importing data count:4400\n",
      "Importing data count:4500\n",
      "Importing data count:4600\n",
      "Importing data count:4700\n",
      "Importing data count:4800\n",
      "Importing data count:4900\n",
      "Importing data count:5000\n",
      "Importing data count:5100\n",
      "Importing data count:5200\n",
      "Importing data count:5300\n",
      "Importing data count:5400\n",
      "Importing data count:5500\n",
      "Importing data count:5600\n",
      "Importing data count:5700\n",
      "Importing data count:5800\n",
      "Importing data count:5900\n",
      "Importing data count:6000\n",
      "Importing data count:6100\n",
      "Importing data count:6200\n",
      "Importing data count:6300\n",
      "Importing data count:6400\n",
      "Importing data count:6500\n",
      "Importing data count:6600\n",
      "Importing data count:6700\n",
      "Importing data count:6800\n",
      "Importing data count:6900\n",
      "Importing data count:7000\n",
      "Importing data count:7100\n",
      "Importing data count:7200\n",
      "Importing data count:7300\n",
      "Importing data count:7400\n",
      "Importing data count:7500\n",
      "Importing data count:7600\n",
      "Importing data count:7700\n",
      "Importing data count:7800\n",
      "============= Start import test data set =============\n",
      "Importing data count:0\n",
      "Importing data count:100\n",
      "Importing data count:200\n",
      "Importing data count:300\n",
      "Importing data count:400\n",
      "Importing data count:500\n",
      "Importing data count:600\n",
      "Importing data count:700\n",
      "Importing data count:800\n"
     ]
    }
   ],
   "source": [
    "dataSet = importData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data:7895\n",
      "Total test data:837\n",
      "Train on 7895 samples, validate on 837 samples\n",
      "Epoch 1/20\n",
      "7895/7895 [==============================] - 74s 9ms/step - loss: 2.7170 - precision: 0.3494 - recall: 0.0307 - val_loss: 2.1568 - val_precision: 0.7734 - val_recall: 0.0346\n",
      "Epoch 2/20\n",
      "7895/7895 [==============================] - 75s 9ms/step - loss: 2.1135 - precision: 0.6491 - recall: 0.0656 - val_loss: 1.9903 - val_precision: 0.7125 - val_recall: 0.0490\n",
      "Epoch 3/20\n",
      "7895/7895 [==============================] - 77s 10ms/step - loss: 1.9165 - precision: 0.6990 - recall: 0.1011 - val_loss: 1.8549 - val_precision: 0.7738 - val_recall: 0.0800\n",
      "Epoch 4/20\n",
      "7895/7895 [==============================] - 76s 10ms/step - loss: 1.7720 - precision: 0.7169 - recall: 0.1273 - val_loss: 1.6900 - val_precision: 0.7271 - val_recall: 0.1338\n",
      "Epoch 5/20\n",
      "7895/7895 [==============================] - 71s 9ms/step - loss: 1.7094 - precision: 0.7230 - recall: 0.1549 - val_loss: 1.6004 - val_precision: 0.8601 - val_recall: 0.1565\n",
      "Epoch 6/20\n",
      "7895/7895 [==============================] - 72s 9ms/step - loss: 1.6054 - precision: 0.7350 - recall: 0.1928 - val_loss: 1.5656 - val_precision: 0.8167 - val_recall: 0.1410\n",
      "Epoch 7/20\n",
      "6400/7895 [=======================>......] - ETA: 13s - loss: 1.5089 - precision: 0.7778 - recall: 0.2186"
     ]
    }
   ],
   "source": [
    "buildModel(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
